{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker Model Monitor - Design a Compelling Record Filtering Method Using Custom Preprocessing Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='overview-0'> </a>\n",
    "## [Overview](./00-Overview.ipynb)\n",
    "* **[Amazon SageMaker Model Monitor- Design a Compelling Record Filtering Method Using Custom Preprocessing Script](./Data_Quality_Custom_Preprocess_Churn.ipynb)**\n",
    "  * **[Business Problem](#business-problem)**\n",
    "  * **[Setup](#nb0-setup)**\n",
    "  * **[Deploy Pre-Trained XGBoost Model with Script-Mode](#nb0-deploy)**\n",
    "  * **[Upload Required Files for SageMaker Model Monitor to S3 Location](#nb0-upload-to-s3)**\n",
    "  * **[Create SageMaker Model Monitoring Schedule (Data Quality only)](#nb0-create-model-monitor)**\n",
    "  * **[Test Scenarios](#nb0-test-scenarios)**\n",
    "  * **[Clean-Up](#nb0-clean-up)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id ='business-problem'> </a>\n",
    "### Business Problem\n",
    "[overview](#overview-0)\n",
    "\n",
    "----\n",
    "Continuous model monitoring and monitor strategy for model retraining and updating are an important step in operationalizing ML. Monitoring can provide information on how the model is performing in production, and the outputs of monitoring can be used to identify the problems proactively and take corrective actions to help stabilization of the model in production. However, in a real-world production settings, multiple personas may interact with the model including real users, engineers who are trouble-shooting production issues, or even bots conducting performance tests. In such a scenario, additional mechanisms may be required to ensure model monitoring works as expected in conjunction with production testing. We will demonstrate how to build a record filtering method based on sets of business criteria as part of preprocessing step in [Amazon SageMaker Model Monitoring](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor.html). The goal is to ensure only the target records are sent to downstream analysis steps to avoid false positive detection of violations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id ='nb0-setup'> </a>\n",
    "### Setup\n",
    "[overview](#overview-0)\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import boto3\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.xgboost.model import XGBoostModel\n",
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "from sagemaker.s3 import S3Uploader, S3Downloader\n",
    "\n",
    "from src.demo_data_quality_model_monitor import DemoDataQualityModelMonitor\n",
    "from src.monitoringjob_utils import run_model_monitor_job_processor\n",
    "from src.artificial_traffic import ArtificialTraffic\n",
    "\n",
    "pd.options.display.max_colwidth = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = get_execution_role()\n",
    "boto_session = boto3.Session()\n",
    "region = boto_session.region_name\n",
    "\n",
    "sm = boto_session.client(\n",
    "    service_name = \"sagemaker\",\n",
    "    region_name = region\n",
    ")\n",
    "s3_client = boto_session.client(\"s3\")\n",
    "\n",
    "project_name = 'DEMO_xgb_churn_prediction_monitor_with_record_filter' #change as needed\n",
    "prefix = f\"sagemaker/{project_name}\" #change as needed\n",
    "ep_prefix = 'DEMO-xgb-churn-pred-ep'\n",
    "data_capture_prefix = f\"{prefix}/datacapture\"\n",
    "s3_capture_upload_path = f\"s3://{bucket}/{data_capture_prefix}\"\n",
    "tags = [{\n",
    "    'Key': 'project',\n",
    "    'Value': 'demo_xgboost_churn_prediction'\n",
    "}]\n",
    "print(f\"project name: {project_name}\")\n",
    "print(f\"project bucket name: {bucket}\")\n",
    "print(f\"project S3 prefix: {prefix}\")\n",
    "print(f\"tags: {tags}\")\n",
    "print(f\"SageMaker DEMO Real-Time Inference Endpoint prefix: {ep_prefix}\")\n",
    "print(f\"SageMaker Model Monitor Data Capture S3 Location: {s3_capture_upload_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id ='nb0-deploy'> </a>\n",
    "### Deploy Pre-Trained XGBoost Model with Script-Mode\n",
    "[overview](#overview-0)\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload a model artifact in a local directory to S3 location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set this flag to True for the first time you run this notebook or when you want to replace the model\n",
    "is_upload_model = True\n",
    "\n",
    "model_path = 'model'\n",
    "model_filename = 'model.tar.gz'\n",
    "model_upload_uri = f's3://{bucket}/{prefix}/{model_path}'\n",
    "local_model_path = f\"./model/{model_filename}\"\n",
    "print(f\"model s3 location: {model_upload_uri} \\n\")\n",
    "\n",
    "if is_upload_model:\n",
    "    S3Uploader.upload(\n",
    "        local_path=local_model_path,\n",
    "        desired_s3_uri=model_upload_uri\n",
    "    )\n",
    "else: print(\"skip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(\n",
    "    HTML(\n",
    "        '<b>Verify <a target=\"blank\" href=\"https://s3.console.aws.amazon.com/s3/buckets/{}?region={}&prefix={}/\">S3 Location </a> After the S3 Copy Has Been Completed</b>'.format(\n",
    "            bucket, region, f'{prefix}/{model_path}'\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Existing Demo Endpoint and Associated Monitor Schedule \n",
    "[Search API](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_Search.html) and Filter by Name and Tags are particularly useful for this purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_results = 10\n",
    "\n",
    "search_params={\n",
    "   \"MaxResults\": max_results,\n",
    "   \"Resource\": \"Endpoint\",\n",
    "   \"SearchExpression\": { \n",
    "      \"Filters\": [\n",
    "          { \n",
    "            \"Name\": f\"Tags.{tags[0].get('Key')}\",\n",
    "            \"Operator\": \"Equals\",\n",
    "            \"Value\": tags[0].get('Value')\n",
    "          },\n",
    "          { \n",
    "            \"Name\": \"EndpointName\",\n",
    "            \"Operator\": \"Contains\",\n",
    "            \"Value\": ep_prefix\n",
    "          },\n",
    "          { \n",
    "            \"Name\": \"MonitoringSchedules.MonitoringScheduleName\",\n",
    "            \"Operator\": \"Contains\",\n",
    "            \"Value\": ep_prefix\n",
    "          }\n",
    "      ]},\n",
    "    \"SortBy\": \"CreationTime\",\n",
    "    \"SortOrder\": \"Descending\"\n",
    "}\n",
    "\n",
    "results = sm.search(**search_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_demo_schedule = []\n",
    "all_demo_eps = []\n",
    "\n",
    "for result in results.get('Results', []):\n",
    "    endpoint = result.get('Endpoint')\n",
    "    if endpoint:\n",
    "        all_demo_eps.append(endpoint['EndpointName'])\n",
    "        mon_schedules = endpoint.get('MonitoringSchedules', [])\n",
    "        for schedule in mon_schedules:\n",
    "            all_demo_schedule.append(schedule['MonitoringScheduleName'])\n",
    "\n",
    "print(f\"Found existing demo schedules: {all_demo_schedule} \") if all_demo_schedule else print(f\"No existing demo schedule containing the prefix,{ep_prefix}, found \")\n",
    "print(f\"Found existing demo endpoints associated with monitor schedule: {all_demo_eps} \") if all_demo_eps else print(f\"No existing endpoint associated with monitor schedule containing the prefix,{ep_prefix}, found \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Existing DEMO SageMaker SageMaker Model Monitor Schedule (by tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set this flag to False for the first time you run this notebook. Set it to True when you want to delete the demo monitor schedule\n",
    "is_rmv_demo_monitor = False\n",
    "\n",
    "if is_rmv_demo_monitor and all_demo_schedule:\n",
    "    print(\"Deleting schedules\", end=\"\", flush=True)\n",
    "    for name in all_demo_schedule:\n",
    "        sm.delete_monitoring_schedule(MonitoringScheduleName=name)\n",
    "        print(\".\", end=\"\", flush=True)\n",
    "        time.sleep(1)\n",
    "else: print(\"skip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Existing DEMO SageMaker Real-Time Inference Endpoints (by tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set this flag to False for the first time you run this notebook. Set it to True when you want to delete the demo endpoints\n",
    "is_rmv_demo_eps = False\n",
    "\n",
    "if is_rmv_demo_eps and all_demo_eps:\n",
    "    print(\"Deleting endpoints\", end=\"\", flush=True)\n",
    "    for ep in all_demo_eps:\n",
    "        sm.delete_endpoint(EndpointName=ep)\n",
    "        print(\".\", end=\"\", flush=True)\n",
    "        time.sleep(1)\n",
    "else: print(\"skip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploy the Model to SageMaker Real-Time Inference Endpoint or Grab the Existing One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pygmentize ./src/inference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify a existing demo inference endpoint name or leave it as a balnk \n",
    "current_endpoint_name = ''\n",
    "\n",
    "# Set this to True if you want to create a new SageMaker Inference Endpoint. Default to True if no demo endpoints w/ monitor schedule found or they have been deleted and endpoint not specified\n",
    "is_create_new_ep = (not(all_demo_eps) or is_rmv_demo_eps) and not(current_endpoint_name)\n",
    "print(f\"Create a new endpoint?: {is_create_new_ep}\")\n",
    "\n",
    "if is_create_new_ep:\n",
    "    ## Configure the Data Capture\n",
    "    data_capture_config = DataCaptureConfig(\n",
    "        enable_capture=True, \n",
    "        sampling_percentage=100, \n",
    "        destination_s3_uri=s3_capture_upload_path\n",
    "    )\n",
    "    current_endpoint_name = f'{ep_prefix}-{datetime.now():%Y-%m-%d-%H-%M}'\n",
    "    print(f\"Create a Endpoint: {current_endpoint_name}\")\n",
    "\n",
    "    xgb_inference_model = XGBoostModel(\n",
    "        model_data=f'{model_upload_uri}/{model_filename}',\n",
    "        role=role,\n",
    "        entry_point=\"./src/inference.py\",\n",
    "        framework_version=\"1.2-1\")\n",
    "    \n",
    "    predictor = xgb_inference_model.deploy(\n",
    "        initial_instance_count=1,\n",
    "        instance_type=\"ml.m5.2xlarge\",\n",
    "        endpoint_name=current_endpoint_name,\n",
    "        data_capture_config=data_capture_config,\n",
    "        tags = tags,\n",
    "        wait=True)\n",
    "elif not(current_endpoint_name):\n",
    "    current_endpoint_name = all_demo_eps[0]\n",
    "    print(f\"Use existing endpoint: {current_endpoint_name}\")  \n",
    "else: print(f\"Use selected endpoint: {current_endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may take a while...please wait until the endpoint creation is complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id ='nb0-upload-to-s3'> </a>\n",
    "### Upload Required Files for SageMaker Model Monitor to S3 Location\n",
    "[overview](#overview-0)\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload the Validation Data including header and label for Sagemaker Model Monitor's Baselining Job \n",
    "SageMaker will suggest a set of constraints as baseline or reference, and generate a set of summary statistics that describe these constraints.\n",
    "The schemas of baseline dataset and the inference dataset should match including the number of features as well as the order of features.\n",
    "We will use the validation dataset that we used to validate the model as a suitable baseline dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_upload_validation_data = True\n",
    "validation_filename = 'validation-dataset-with-header.csv'\n",
    "local_validation_data_path = f\"data/{validation_filename}\"\n",
    "s3_validation_data_uri = f's3://{bucket}/{prefix}/baselining'\n",
    "\n",
    "if is_upload_validation_data:\n",
    "    S3Uploader.upload(\n",
    "        local_path=local_validation_data_path,\n",
    "        desired_s3_uri=s3_validation_data_uri\n",
    "    )\n",
    "else: print(\"skip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(\n",
    "    HTML(\n",
    "        '<b>Verify <a target=\"blank\" href=\"https://s3.console.aws.amazon.com/s3/buckets/{}?region={}&prefix={}/\">S3 Upload </a> After the S3 Copy Has Been Completed</b>'.format(\n",
    "            bucket, region, f'{prefix}/baselining'\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload the Custom Preprocessing Script to to the S3 Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pygmentize ./src/preprocessor.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_upload_preprocess_script = True\n",
    "\n",
    "preprocessor_filename = 'preprocessor.py'\n",
    "local_path_preprocessor = f\"src/{preprocessor_filename}\"\n",
    "s3_record_preprocessor_uri = f's3://{bucket}/{prefix}/code'\n",
    "\n",
    "if is_upload_preprocess_script:\n",
    "    S3Uploader.upload(\n",
    "        local_path=local_path_preprocessor,\n",
    "        desired_s3_uri=s3_record_preprocessor_uri\n",
    "    )\n",
    "else: print(\"skip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(\n",
    "    HTML(\n",
    "        '<b>Verify <a target=\"blank\" href=\"https://s3.console.aws.amazon.com/s3/buckets/{}?region={}&prefix={}/\">S3 Upload </a> After the S3 Copy Has Been Completed</b>'.format(\n",
    "            bucket, region, f'{prefix}/code'\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id ='nb0-create-model-monitor'> </a>\n",
    "### Create SageMaker Model Monitoring Schedule (Data Quality only)\n",
    "[overview](#overview-0)\n",
    "\n",
    "----\n",
    "We will create baseline constraints and statistics and model monitoring schedule for the Endpoint in one go using the custom utility tool.\n",
    "Under the hood, [DefaultModelMonitor class](https://sagemaker.readthedocs.io/en/stable/api/inference/model_monitor.html) is used to kick off SageMaker Processing Job with a SageMaker-provided Model Monitor Docker container with Apache Spark and the AWS Deequ open source library to generate the constraints and statistics as a baseline. After the baselining job completes, a monitoring schedule will be created given the parameters you specified below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pygmentize ./src/demo_data_quality_model_monitor.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_mon = DemoDataQualityModelMonitor(\n",
    "    endpoint_name=current_endpoint_name, \n",
    "    bucket=bucket,\n",
    "    projectfolder_prefix=prefix,\n",
    "    training_dataset_path=f'{s3_validation_data_uri}/{validation_filename}',\n",
    "    record_preprocessor_script=f'{s3_record_preprocessor_uri}/{preprocessor_filename}',\n",
    "    post_analytics_processor_script=None,\n",
    "    kms_key=None,\n",
    "    subnets=None,\n",
    "    security_group_ids=None,\n",
    "    role=role,\n",
    "    tags=tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Data Quality Monitor Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_monitor = demo_mon.create_data_quality_monitor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may take a while.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Outputs of Baseline Suggestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_data_quality_prefix = f'{prefix}/data_quality'\n",
    "s3_data_quality_baseline_prefix = f'{s3_data_quality_prefix}/baselining'\n",
    "\n",
    "# Get a list of S3 URIs\n",
    "report_files = S3Downloader.list(f\"s3://{bucket}/{s3_data_quality_baseline_prefix}\")\n",
    "pd.DataFrame(json.loads(S3Downloader.read_file(report_files[0]))[\"features\"])\n",
    "\n",
    "for filename in report_files:\n",
    "    if str(filename).__contains__('statistics.json'):\n",
    "        s3_statistics_uri = filename\n",
    "        schema_df = pd.json_normalize(json.loads(S3Downloader.read_file(s3_statistics_uri))[\"features\"])\n",
    "    elif str(filename).__contains__('constraints.json'):\n",
    "        s3_constraints_uri = filename\n",
    "        constraints_df = pd.json_normalize(json.loads(S3Downloader.read_file(s3_constraints_uri))[\"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id ='nb0-test-scenarios'> </a>\n",
    "### Test Scenarios\n",
    "[overview](#overview-0)\n",
    "\n",
    "----\n",
    "We will test a few scenarios to verify if filtering based on custom attributes is working"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Scenario: \n",
    "   1. Send a record that we know won't trigger any violations. To do this, you can use a method, `generate_artifical_traffic` and set `config` variable to empty list. Also set the `testIndicator` in custom attributes to `false` to indicate that itâ€™s not a test record. \n",
    "   2. Send another record that would actually trigger a violation. This time, we pass a set of dictionaries in `config` variable create bogus input features as shown below, and also set `testIndicator` to `true` to skip this record for the analysis. \n",
    "   3. Manually kick off a monitor job using `run_model_monitor_job_processor` method from the imported utility class and provide parameters such as s3 locations for baseline files, data capture, preprocessor script, and other info.\n",
    "   4. In outputs of Monitor, confirm that `constraint_violations.json` shows `violations: [] 0 items` and `dataset: item_count:` in `statistics.json` shows `1`, instead of `2`. \n",
    "   5. This would confirm that Model Monitor has analyzed only the non-test record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pygmentize ./src/artificial_traffic.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artificial_traffic = ArtificialTraffic(\n",
    "    endpointName = current_endpoint_name\n",
    ")\n",
    "print(f'EndpointName: {artificial_traffic.endpointName}')\n",
    "print(f'transaction_id: {artificial_traffic.transactionId}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = json.load(open(f'./data/sample.json','r'))\n",
    "sample_config = json.load(open(f'./data/config.json','r'))\n",
    "sample_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload=json.load(open(f'./data/sample.json','r'))\n",
    "\n",
    "# normal payload -it should not cause any violations\n",
    "artificial_traffic.generate_artificial_traffic(\n",
    "    applicationName = \"DEMO\", \n",
    "    testIndicator = \"false\",\n",
    "    payload=payload, \n",
    "    size=1,\n",
    "    config=[]\n",
    ")\n",
    "\n",
    "## this would cause violations but testIndicaor is set to true so analysis will be skipped and hence no violations\n",
    "artificial_traffic.generate_artificial_traffic(\n",
    "    applicationName=\"DEMO\", \n",
    "    testIndicator=\"true\",\n",
    "    payload=payload, \n",
    "    size=1,\n",
    "    config=sample_config['config']\n",
    ")\n",
    "print(f\"Current Transaction Id: {artificial_traffic.transactionId}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View Data Capture file in S3\n",
    "It may take a minute for data capture files to be populated in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_endpoint_capture_prefix = f\"{data_capture_prefix}/{current_endpoint_name}\"\n",
    "capture_files_scenario_1  = S3Downloader.list(f\"s3://{bucket}/{current_endpoint_capture_prefix}\")\n",
    "\n",
    "while len(capture_files_scenario_1) == 0:\n",
    "    capture_files_scenario_1  = S3Downloader.list(f\"s3://{bucket}/{current_endpoint_capture_prefix}\")\n",
    "    if len(capture_files_scenario_1) == 0:\n",
    "        time.sleep(10)\n",
    "\n",
    "data_capture_path_scenario_1 = capture_files_scenario_1[len(capture_files_scenario_1) - 1][: capture_files_scenario_1[len(capture_files_scenario_1) - 1].rfind('/')]\n",
    "print(f\"\\n data capture path: {data_capture_path_scenario_1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trigger a Manual Model Monitoring Job \n",
    "SageMaker Model Monitor uses [Processing Job](https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job.html) under the hood so we can manually trigger a Monitoring job for testing. Fortunately, there is a utility tool which is available from this [repository](https://github.com/aws-samples/reinvent2019-aim362-sagemaker-debugger-model-monitor/tree/master/02_deploy_and_monitor) that already implements it for us. We will import this utility tool to trigger a manual Model Monitor job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pygmentize ./src/monitoringjob_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_reports_path = f's3://{bucket}/{prefix}/reports'\n",
    "\n",
    "print(f\"S3 Location for statistics.json: {s3_statistics_uri}\")\n",
    "print(f\"S3 Location for constraints.json: {s3_constraints_uri}\")\n",
    "print(f\"S3 Location for report outputs: {s3_reports_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model_monitor_job_processor(\n",
    "    region,\n",
    "    'ml.m5.xlarge',\n",
    "    role,\n",
    "    data_capture_path_scenario_1,\n",
    "    s3_statistics_uri,\n",
    "    s3_constraints_uri,\n",
    "    s3_reports_path+'/scenario_1',\n",
    "    preprocessor_path=f'{s3_record_preprocessor_uri}/{preprocessor_filename}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the Manual Monitor Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_monitor_job = sm.list_processing_jobs(\n",
    "    NameContains = 'sagemaker-model-monitor-analyzer',\n",
    "    SortOrder='Descending',\n",
    "    MaxResults=2\n",
    ")['ProcessingJobSummaries'][0]['ProcessingJobName']\n",
    "\n",
    "manual_monitoring_job_info = sm.describe_processing_job(\n",
    "    ProcessingJobName=manual_monitor_job\n",
    ")\n",
    "\n",
    "manual_monitoring_job_output = manual_monitoring_job_info['ProcessingOutputConfig']['Outputs'][0]['S3Output']['S3Uri']\n",
    "\n",
    "print(manual_monitoring_job_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls $manual_monitoring_job_output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_json(f'{manual_monitoring_job_output}/constraint_violations.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm above that there is no violations detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp $manual_monitoring_job_output/statistics.json - | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm that \"item_count\" is 1 not 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Scenario:        \n",
    "   1. Send N records that we know that would trigger violations such as `data_type_check` and `baseline_drift_check`. set the `testIndicator` in custom attributes to `false`. \n",
    "   2. In Monitor outputs, confirm that `constraint_violations.json` shows `violations: [] 2 items` and `dataset: item_count:` in `statistics.json` shows `1001`. An extra item is a carry over from the first scenario testing so this is expected.  \n",
    "   3. This would confirm that sending test records as inference records would trigger false positive violations if `testIndicator` is not set correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artificial_traffic.generate_artificial_traffic(\n",
    "    applicationName=\"DEMO\", \n",
    "    testIndicator=\"false\",\n",
    "    payload=payload, \n",
    "    size=1000,\n",
    "    config=sample_config['config']\n",
    ")\n",
    "print(f\"Current Transaction Id: {artificial_traffic.transactionId}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_scenario_2 = s3_client.list_objects(Bucket=bucket, Prefix=current_endpoint_capture_prefix)\n",
    "capture_files_scenario_2  = ['s3://{0}/{1}'.format(bucket, capture_file.get(\"Key\")) for capture_file in result_scenario_2.get('Contents')]\n",
    "\n",
    "print(\"Capture Files: \")\n",
    "print(\"\\n \".join(capture_files_scenario_2))\n",
    "\n",
    "data_capture_path_scenario_2 = capture_files_scenario_2[len(capture_files_scenario_2) - 1][: capture_files_scenario_2[len(capture_files_scenario_2) - 1].rfind('/')]\n",
    "print(f\"\\n data capture path: {data_capture_path_scenario_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model_monitor_job_processor(region, \n",
    "                                'ml.m5.xlarge', \n",
    "                                role, \n",
    "                                data_capture_path_scenario_2, \n",
    "                                s3_statistics_uri, \n",
    "                                s3_constraints_uri, \n",
    "                                s3_reports_path+'/scenario_2',\n",
    "                                preprocessor_path=f'{s3_record_preprocessor_uri}/{preprocessor_filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_monitor_job = sm.list_processing_jobs(\n",
    "    NameContains = 'sagemaker-model-monitor-analyzer',\n",
    "    SortOrder='Descending',\n",
    "    MaxResults=2\n",
    ")['ProcessingJobSummaries'][0]['ProcessingJobName']\n",
    "\n",
    "manual_monitoring_job_info = sm.describe_processing_job(\n",
    "    ProcessingJobName=manual_monitor_job\n",
    ")\n",
    "\n",
    "manual_monitoring_job_output = manual_monitoring_job_info['ProcessingOutputConfig']['Outputs'][0]['S3Output']['S3Uri']\n",
    "\n",
    "print(manual_monitoring_job_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls $manual_monitoring_job_output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_json(f'{manual_monitoring_job_output}/constraint_violations.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm that there are violations detected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp $manual_monitoring_job_output/statistics.json - | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id ='nb0-clean-up'> </a>\n",
    "### Clean-Up\n",
    "[overview](#overview-0)\n",
    "\n",
    "----\n",
    "We can delete model monitoring schedule and endpoint we created earlier. You can wait to run the following code until the scheduled monitor has been kicked off if you are interested. You should expect to see a similar results we reviewed from a monitor job that we kicked off manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_monitor.delete_monitoring_schedule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.delete_endpoint(EndpointName=current_endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Release Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "\n",
    "<p><b>Shutting down your kernel for this notebook to release resources.</b></p>\n",
    "<button class=\"sm-command-button\" data-commandlinker-command=\"kernelmenu:shutdown\" style=\"display:none;\">Shutdown Kernel</button>\n",
    "        \n",
    "<script>\n",
    "try {\n",
    "    els = document.getElementsByClassName(\"sm-command-button\");\n",
    "    els[0].click();\n",
    "}\n",
    "catch(err) {}    \n",
    "</script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
